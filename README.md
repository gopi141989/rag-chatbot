# ğŸ“„ Local PDF RAG Chatbot (Offline)

A **100% offline PDF chatbot** built using **Streamlit, LangChain, FAISS, HuggingFace embeddings**, and a **local LLM (LLaMA/Mistral via llama.cpp)**.  
Ask questions from a PDF without any paid APIs.

---

## ğŸš€ Features

- ğŸ“š Question answering from PDFs
- ğŸ”’ Fully offline (no API keys)
- ğŸ§  Semantic search using FAISS
- ğŸ¦™ Local LLM with llama.cpp
- ğŸ–¥ï¸ Simple Streamlit UI

---

## ğŸ—ï¸ Architecture

User
â”‚
â–¼
Streamlit UI
â”‚
â–¼
LangChain RetrievalQA
â”‚
â”œâ”€â”€ FAISS Vector DB
â”‚ â””â”€â”€ HuggingFace Embeddings
â”‚
â””â”€â”€ Local LLM (LLaMA / Mistral)
via llama.cpp


---

## ğŸ—‚ï¸ Project Structure

project/
â”‚â”€â”€ chatbot.py
â”‚â”€â”€ book.pdf
â”‚â”€â”€ README.md
â”‚â”€â”€ models/
â”‚ â””â”€â”€ ggml-model-q4_0.bin


> âš ï¸ Model files are not committed to GitHub.

---

## ğŸ› ï¸ Requirements

- Python 3.9 â€“ 3.11
- Minimum 8 GB RAM
- CPU only (no GPU required)

---

## ğŸ“¦ Installation

```bash
git clone https://github.com/your-username/your-repo.git
cd your-repo
python -m venv venv
venv\Scripts\activate   # Windows
pip install langchain==0.2.14 langchain-core==0.2.38 langchain-community==0.2.12
pip install langchain-text-splitters faiss-cpu llama-cpp-python streamlit
pip install sentence-transformers pypdf

ğŸ§  Download Model

Download a GGML/GGUF model (example):

mistral-7b-instruct-v0.1.Q4_0.bin

Place it here:

models/ggml-model-q4_0.bin

â–¶ï¸ Run App
streamlit run chatbot.py


Open: http://localhost:8501

ğŸ“¸ Screenshots

Add screenshots here after running the app:

screenshots/
â”‚â”€â”€ home.png
â”‚â”€â”€ answer.png

![Home](screenshots/home.png)
![Answer](screenshots/answer.png)

ğŸ§ª Troubleshooting
âŒ ModuleNotFoundError

âœ”ï¸ Ensure correct LangChain versions are installed
âœ”ï¸ Activate virtual environment

âŒ Model not loading / app crashes

âœ”ï¸ Use Q4 model for low RAM
âœ”ï¸ Check correct MODEL_PATH

âŒ Slow responses

âœ”ï¸ Reduce chunk_size
âœ”ï¸ Use smaller model (TinyLLaMA / Phi-2)

âŒ GitHub push fails (large file)

âœ”ï¸ Add to .gitignore:

models/
*.bin
venv/

ğŸ“Œ Notes

Works fully offline after setup

CPU inference is slower than cloud APIs

Ideal for learning RAG fundamentals

ğŸ“œ License

For educational use.
Follow the license of the downloaded LLM model.


---
